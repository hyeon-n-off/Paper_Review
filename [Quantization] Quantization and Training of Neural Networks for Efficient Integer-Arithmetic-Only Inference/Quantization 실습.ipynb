{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import CIFAR100\n",
    " \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "from torchvision.models import mobilenet_v2\n",
    "import torch.quantization\n",
    "from torch.quantization import QuantStub, DeQuantStub\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "train_dataset = CIFAR100(root='./data', train=True, transform=transform, download=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "\n",
    "test_dataset = CIFAR100(root='./data', train=False, transform=transform, download=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvertedBottleneck(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, t, stride = 1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.stride = stride\n",
    "        self.add = nn.quantized.FloatFunctional()\n",
    "\n",
    "        expand = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels * t, 1, bias = False),\n",
    "            nn.BatchNorm2d(in_channels * t),\n",
    "            nn.ReLU6(inplace = True),\n",
    "        )\n",
    "        depthwise = nn.Sequential(\n",
    "            nn.Conv2d(in_channels * t, in_channels * t, 3, stride = stride, padding = 1, groups = in_channels * t, bias = False),\n",
    "            nn.BatchNorm2d(in_channels * t),\n",
    "            nn.ReLU6(inplace = True),\n",
    "        )\n",
    "        pointwise = nn.Sequential(\n",
    "            nn.Conv2d(in_channels * t, out_channels, 1, bias = False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "        )\n",
    "        \n",
    "        residual_list = []\n",
    "        if t > 1:\n",
    "            residual_list += [expand]\n",
    "        residual_list += [depthwise, pointwise]\n",
    "        self.residual = nn.Sequential(*residual_list)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.stride == 1 and self.in_channels == self.out_channels:\n",
    "            out = self.add.add(self.residual(x), x)\n",
    "        else:\n",
    "            out = self.residual(x)\n",
    "    \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantizedMobileNetV2(nn.Module):\n",
    "    def __init__(self, n_classes = 100):\n",
    "        super().__init__()\n",
    "\n",
    "        self.quant = QuantStub()\n",
    "        self.dequant = DeQuantStub()\n",
    "\n",
    "        self.first_conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, stride = 2, padding = 1, bias = False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU6(inplace = True)\n",
    "        )\n",
    "\n",
    "        self.bottlenecks = nn.Sequential(\n",
    "            self.make_stage(32, 16, t = 1, n = 1),\n",
    "            self.make_stage(16, 24, t = 6, n = 2, stride = 2),\n",
    "            self.make_stage(24, 32, t = 6, n = 3, stride = 2),\n",
    "            self.make_stage(32, 64, t = 6, n = 4, stride = 2),\n",
    "            self.make_stage(64, 96, t = 6, n = 3),\n",
    "            self.make_stage(96, 160, t = 6, n = 3, stride = 2),\n",
    "            self.make_stage(160, 320, t = 6, n = 1)\n",
    "        )\n",
    "\n",
    "        self.last_conv = nn.Sequential(\n",
    "            nn.Conv2d(320, 1280, 1, bias = False),\n",
    "            nn.BatchNorm2d(1280),\n",
    "            nn.ReLU6(inplace = True)\n",
    "        )\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Sequential(\n",
    "        \tnn.Dropout(0.2), # 채널 축으로 놓여있는 feature 들을 일부 가려보면서 학습\n",
    "            nn.Linear(1280, n_classes),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "\n",
    "        x = self.first_conv(x)\n",
    "        x = self.bottlenecks(x)\n",
    "        x = self.last_conv(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1) # (N, C, 1, 1) -> (N, C)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        x = self.dequant(x)\n",
    "        return x\n",
    "    \n",
    "    def make_stage(self, in_channels, out_channels, t, n, stride = 1):\n",
    "        layers = [InvertedBottleneck(in_channels, out_channels, t, stride)]\n",
    "        in_channels = out_channels\n",
    "        for _ in range(n-1):\n",
    "            layers.append(InvertedBottleneck(in_channels, out_channels, t))\n",
    "        \n",
    "        return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습 함수\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        tepoch = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\", unit=\"batch\")\n",
    "        \n",
    "        for inputs, labels in tepoch:\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            tepoch.set_postfix(loss=running_loss / (tepoch.n or 1))\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "# 모델 평가 함수\n",
    "def evaluate_model(model, test_loader, is_cuda=True):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        ttest = tqdm(test_loader, desc=\"Evaluating\", unit=\"batch\")\n",
    "\n",
    "        for inputs, labels in ttest:\n",
    "            if is_cuda:\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "            else:\n",
    "                inputs, labels = inputs.cpu(), labels.cpu()\n",
    "                \n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = correct / total\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}% | correct / total: {correct} / {total}\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_dataset = Subset(train_dataset, range(1000))\n",
    "sub_loader = DataLoader(sub_dataset, batch_size=32, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the quantized model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36f341b0692d457ab9befbbcdb12318e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/1:   0%|          | 0/32 [00:17<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 4.676937982439995\n"
     ]
    }
   ],
   "source": [
    "print(\"Training the quantized model...\")\n",
    "quantized_model = QuantizedMobileNetV2(n_classes=100).cuda()\n",
    "\n",
    "# 양자화 준비\n",
    "quantized_model.qconfig = torch.quantization.QConfig(\n",
    "    weight=torch.quantization.MinMaxObserver.with_args(dtype=torch.qint8),\n",
    "    activation=torch.quantization.MovingAverageMinMaxObserver.with_args(dtype=torch.quint8),\n",
    ")\n",
    "\n",
    "torch.quantization.prepare_qat(quantized_model, inplace=True)  # QAT 양자화\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(quantized_model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "train_model(quantized_model, sub_loader, criterion, optimizer, num_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MSI\\anaconda3\\envs\\NLP\\Lib\\site-packages\\torch\\ao\\quantization\\utils.py:339: UserWarning: must run observer before calling calculate_qparams. Returning default values.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "QuantizedMobileNetV2(\n",
       "  (quant): Quantize(scale=tensor([0.0187]), zero_point=tensor([114]), dtype=torch.quint8)\n",
       "  (dequant): DeQuantize()\n",
       "  (first_conv): Sequential(\n",
       "    (0): QuantizedConv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), scale=0.06207526847720146, zero_point=136, padding=(1, 1), bias=False)\n",
       "    (1): QuantizedBatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): QuantizedReLU6(inplace=True)\n",
       "  )\n",
       "  (bottlenecks): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): InvertedBottleneck(\n",
       "        (add): QFunctional(\n",
       "          scale=1.0, zero_point=0\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "        (residual): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): QuantizedConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.03298689052462578, zero_point=127, padding=(1, 1), groups=32, bias=False)\n",
       "            (1): QuantizedBatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): QuantizedReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): QuantizedConv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), scale=0.02587594836950302, zero_point=132, bias=False)\n",
       "            (1): QuantizedBatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): InvertedBottleneck(\n",
       "        (add): QFunctional(\n",
       "          scale=1.0, zero_point=0\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "        (residual): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): QuantizedConv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), scale=0.05429937317967415, zero_point=124, bias=False)\n",
       "            (1): QuantizedBatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): QuantizedReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): QuantizedConv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), scale=0.042721617966890335, zero_point=129, padding=(1, 1), groups=96, bias=False)\n",
       "            (1): QuantizedBatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): QuantizedReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): QuantizedConv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), scale=0.03238290548324585, zero_point=125, bias=False)\n",
       "            (1): QuantizedBatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): InvertedBottleneck(\n",
       "        (add): QFunctional(\n",
       "          scale=0.09280647337436676, zero_point=129\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "        (residual): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): QuantizedConv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), scale=0.047308243811130524, zero_point=131, bias=False)\n",
       "            (1): QuantizedBatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): QuantizedReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): QuantizedConv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), scale=0.04871309921145439, zero_point=130, padding=(1, 1), groups=144, bias=False)\n",
       "            (1): QuantizedBatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): QuantizedReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): QuantizedConv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), scale=0.03070848621428013, zero_point=130, bias=False)\n",
       "            (1): QuantizedBatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): InvertedBottleneck(\n",
       "        (add): QFunctional(\n",
       "          scale=1.0, zero_point=0\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "        (residual): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): QuantizedConv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), scale=0.0805143490433693, zero_point=132, bias=False)\n",
       "            (1): QuantizedBatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): QuantizedReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): QuantizedConv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), scale=0.04257994145154953, zero_point=127, padding=(1, 1), groups=144, bias=False)\n",
       "            (1): QuantizedBatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): QuantizedReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): QuantizedConv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), scale=0.029968025162816048, zero_point=118, bias=False)\n",
       "            (1): QuantizedBatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): InvertedBottleneck(\n",
       "        (add): QFunctional(\n",
       "          scale=0.08130742609500885, zero_point=133\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "        (residual): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): QuantizedConv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), scale=0.045111414045095444, zero_point=126, bias=False)\n",
       "            (1): QuantizedBatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): QuantizedReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): QuantizedConv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), scale=0.03971375897526741, zero_point=125, padding=(1, 1), groups=192, bias=False)\n",
       "            (1): QuantizedBatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): QuantizedReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): QuantizedConv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), scale=0.022759171202778816, zero_point=130, bias=False)\n",
       "            (1): QuantizedBatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): InvertedBottleneck(\n",
       "        (add): QFunctional(\n",
       "          scale=0.10232627391815186, zero_point=129\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "        (residual): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): QuantizedConv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), scale=0.054669544100761414, zero_point=120, bias=False)\n",
       "            (1): QuantizedBatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): QuantizedReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): QuantizedConv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), scale=0.03556264564394951, zero_point=127, padding=(1, 1), groups=192, bias=False)\n",
       "            (1): QuantizedBatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): QuantizedReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): QuantizedConv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), scale=0.024677297100424767, zero_point=127, bias=False)\n",
       "            (1): QuantizedBatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): InvertedBottleneck(\n",
       "        (add): QFunctional(\n",
       "          scale=1.0, zero_point=0\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "        (residual): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): QuantizedConv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), scale=0.06992392987012863, zero_point=130, bias=False)\n",
       "            (1): QuantizedBatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): QuantizedReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): QuantizedConv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), scale=0.036999594420194626, zero_point=122, padding=(1, 1), groups=192, bias=False)\n",
       "            (1): QuantizedBatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): QuantizedReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): QuantizedConv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), scale=0.022358449175953865, zero_point=135, bias=False)\n",
       "            (1): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): InvertedBottleneck(\n",
       "        (add): QFunctional(\n",
       "          scale=0.07808740437030792, zero_point=140\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "        (residual): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): QuantizedConv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), scale=0.04080400615930557, zero_point=123, bias=False)\n",
       "            (1): QuantizedBatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): QuantizedReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): QuantizedConv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), scale=0.03177998214960098, zero_point=128, padding=(1, 1), groups=384, bias=False)\n",
       "            (1): QuantizedBatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): QuantizedReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): QuantizedConv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), scale=0.020360996946692467, zero_point=139, bias=False)\n",
       "            (1): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): InvertedBottleneck(\n",
       "        (add): QFunctional(\n",
       "          scale=0.09392890334129333, zero_point=124\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "        (residual): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): QuantizedConv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), scale=0.056740909814834595, zero_point=123, bias=False)\n",
       "            (1): QuantizedBatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): QuantizedReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): QuantizedConv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), scale=0.03429129347205162, zero_point=131, padding=(1, 1), groups=384, bias=False)\n",
       "            (1): QuantizedBatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): QuantizedReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): QuantizedConv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), scale=0.022228876128792763, zero_point=131, bias=False)\n",
       "            (1): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): InvertedBottleneck(\n",
       "        (add): QFunctional(\n",
       "          scale=0.1038573607802391, zero_point=127\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "        (residual): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): QuantizedConv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), scale=0.06838168203830719, zero_point=121, bias=False)\n",
       "            (1): QuantizedBatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): QuantizedReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): QuantizedConv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), scale=0.042374271899461746, zero_point=104, padding=(1, 1), groups=384, bias=False)\n",
       "            (1): QuantizedBatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): QuantizedReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): QuantizedConv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), scale=0.02021823823451996, zero_point=136, bias=False)\n",
       "            (1): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): InvertedBottleneck(\n",
       "        (add): QFunctional(\n",
       "          scale=1.0, zero_point=0\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "        (residual): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): QuantizedConv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), scale=0.07091344892978668, zero_point=130, bias=False)\n",
       "            (1): QuantizedBatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): QuantizedReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): QuantizedConv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), scale=0.03842717781662941, zero_point=140, padding=(1, 1), groups=384, bias=False)\n",
       "            (1): QuantizedBatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): QuantizedReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): QuantizedConv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), scale=0.020316243171691895, zero_point=132, bias=False)\n",
       "            (1): QuantizedBatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): InvertedBottleneck(\n",
       "        (add): QFunctional(\n",
       "          scale=0.07293593138456345, zero_point=131\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "        (residual): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): QuantizedConv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), scale=0.03669969365000725, zero_point=129, bias=False)\n",
       "            (1): QuantizedBatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): QuantizedReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): QuantizedConv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), scale=0.036636460572481155, zero_point=140, padding=(1, 1), groups=576, bias=False)\n",
       "            (1): QuantizedBatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): QuantizedReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): QuantizedConv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), scale=0.02059059962630272, zero_point=133, bias=False)\n",
       "            (1): QuantizedBatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): InvertedBottleneck(\n",
       "        (add): QFunctional(\n",
       "          scale=0.095423623919487, zero_point=130\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "        (residual): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): QuantizedConv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), scale=0.0520792193710804, zero_point=122, bias=False)\n",
       "            (1): QuantizedBatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): QuantizedReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): QuantizedConv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), scale=0.03523115813732147, zero_point=138, padding=(1, 1), groups=576, bias=False)\n",
       "            (1): QuantizedBatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): QuantizedReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): QuantizedConv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), scale=0.02098982408642769, zero_point=136, bias=False)\n",
       "            (1): QuantizedBatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): InvertedBottleneck(\n",
       "        (add): QFunctional(\n",
       "          scale=1.0, zero_point=0\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "        (residual): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): QuantizedConv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), scale=0.062415365129709244, zero_point=130, bias=False)\n",
       "            (1): QuantizedBatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): QuantizedReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): QuantizedConv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), scale=0.03247368335723877, zero_point=119, padding=(1, 1), groups=576, bias=False)\n",
       "            (1): QuantizedBatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): QuantizedReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): QuantizedConv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), scale=0.020734913647174835, zero_point=128, bias=False)\n",
       "            (1): QuantizedBatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): InvertedBottleneck(\n",
       "        (add): QFunctional(\n",
       "          scale=0.0693838819861412, zero_point=130\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "        (residual): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): QuantizedConv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), scale=0.03471897915005684, zero_point=131, bias=False)\n",
       "            (1): QuantizedBatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): QuantizedReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): QuantizedConv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), scale=0.03073064796626568, zero_point=128, padding=(1, 1), groups=960, bias=False)\n",
       "            (1): QuantizedBatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): QuantizedReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): QuantizedConv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), scale=0.019267287105321884, zero_point=136, bias=False)\n",
       "            (1): QuantizedBatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): InvertedBottleneck(\n",
       "        (add): QFunctional(\n",
       "          scale=0.08121828734874725, zero_point=132\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "        (residual): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): QuantizedConv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), scale=0.04694199562072754, zero_point=126, bias=False)\n",
       "            (1): QuantizedBatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): QuantizedReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): QuantizedConv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), scale=0.029881665483117104, zero_point=135, padding=(1, 1), groups=960, bias=False)\n",
       "            (1): QuantizedBatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): QuantizedReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): QuantizedConv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), scale=0.018538400530815125, zero_point=130, bias=False)\n",
       "            (1): QuantizedBatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): InvertedBottleneck(\n",
       "        (add): QFunctional(\n",
       "          scale=1.0, zero_point=0\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "        (residual): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): QuantizedConv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), scale=0.05674681439995766, zero_point=134, bias=False)\n",
       "            (1): QuantizedBatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): QuantizedReLU6(inplace=True)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): QuantizedConv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), scale=0.030518436804413795, zero_point=122, padding=(1, 1), groups=960, bias=False)\n",
       "            (1): QuantizedBatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): QuantizedReLU6(inplace=True)\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): QuantizedConv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), scale=0.020295629277825356, zero_point=123, bias=False)\n",
       "            (1): QuantizedBatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (last_conv): Sequential(\n",
       "    (0): QuantizedConv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), scale=0.03409649431705475, zero_point=120, bias=False)\n",
       "    (1): QuantizedBatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): QuantizedReLU6(inplace=True)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Sequential(\n",
       "    (0): QuantizedDropout(p=0.2, inplace=False)\n",
       "    (1): QuantizedLinear(in_features=1280, out_features=100, scale=0.010643825866281986, zero_point=110, qscheme=torch.per_tensor_affine)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 양자화 후 모델 변환\n",
    "torch.quantization.convert(quantized_model.cpu(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the quantized model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f4d16c745194f239ff62efe40b402b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/313 [00:14<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.08% | correct / total: 108 / 10000\n",
      "Quantized Model Accuracy: 1.08%\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating the quantized model...\")\n",
    "quantized_accuracy = evaluate_model(quantized_model.cpu(), test_loader, is_cuda=False)\n",
    "\n",
    "print(f\"Quantized Model Accuracy: {quantized_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and evaluating the original model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MSI\\anaconda3\\envs\\NLP\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\MSI\\anaconda3\\envs\\NLP\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training and evaluating the original model...\")\n",
    "original_model = mobilenet_v2(pretrained=False)\n",
    "original_model.classifier[1] = nn.Linear(original_model.last_channel, 100)  # CIFAR100에 맞게 수정\n",
    "original_model = original_model.cuda()\n",
    "\n",
    "train_model(original_model, train_loader, criterion, optimizer, num_epochs=5)\n",
    "original_accuracy = evaluate_model(original_model, test_loader)\n",
    "\n",
    "print(f\"Original Model Accuracy: {original_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    original_model,\n",
    "    {torch.nn.Linear, torch.nn.Conv2d},\n",
    "    dtype=torch.qint8\n",
    ")\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
